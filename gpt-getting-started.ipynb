{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple overview of the OpenAI SDK in Python\n",
    "\n",
    "Note: You need to set your API Key first in your environment like so:\n",
    "\n",
    "`export OPENAI_API_KEY=<api_key>`\n",
    "\n",
    "And of course install [the Library](https://github.com/openai/openai-python):\n",
    "\n",
    "`pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage\n",
      "davinci\n",
      "babbage-code-search-code\n",
      "text-similarity-babbage-001\n",
      "text-davinci-001\n",
      "ada\n",
      "curie-instruct-beta\n",
      "babbage-code-search-text\n",
      "babbage-similarity\n",
      "whisper-1\n",
      "code-search-babbage-text-001\n",
      "text-curie-001\n",
      "code-search-babbage-code-001\n",
      "text-ada-001\n",
      "text-embedding-ada-002\n",
      "text-similarity-ada-001\n",
      "ada-code-search-code\n",
      "ada-similarity\n",
      "text-davinci-003\n",
      "code-search-ada-text-001\n",
      "text-search-ada-query-001\n",
      "davinci-search-document\n",
      "ada-code-search-text\n",
      "text-search-ada-doc-001\n",
      "davinci-instruct-beta\n",
      "text-similarity-curie-001\n",
      "code-search-ada-code-001\n",
      "ada-search-query\n",
      "text-search-davinci-query-001\n",
      "curie-search-query\n",
      "gpt-3.5-turbo-0301\n",
      "davinci-search-query\n",
      "babbage-search-document\n",
      "ada-search-document\n",
      "text-search-curie-query-001\n",
      "text-search-babbage-doc-001\n",
      "gpt-3.5-turbo\n",
      "curie-search-document\n",
      "text-search-curie-doc-001\n",
      "babbage-search-query\n",
      "text-babbage-001\n",
      "text-search-davinci-doc-001\n",
      "text-search-babbage-query-001\n",
      "curie-similarity\n",
      "curie\n",
      "text-similarity-davinci-001\n",
      "text-davinci-002\n",
      "davinci-similarity\n",
      "cushman:2020-05-03\n",
      "ada:2020-05-03\n",
      "babbage:2020-05-03\n",
      "curie:2020-05-03\n",
      "davinci:2020-05-03\n",
      "if-davinci-v2\n",
      "if-curie-v2\n",
      "if-davinci:3.0.0\n",
      "davinci-if:3.0.0\n",
      "davinci-instruct-beta:2.0.0\n",
      "text-ada:001\n",
      "text-davinci:001\n",
      "text-curie:001\n",
      "text-babbage:001\n",
      "Sorry, not today.\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import openai\n",
    "\n",
    "# list models\n",
    "models = openai.Model.list()\n",
    "\n",
    "# Now let's take a look at the models and see if we have GPT-4 yet or not.\n",
    "gpt4 = False\n",
    "for model in models.data: \n",
    "  if 'gpt-4' in model[\"id\"]:\n",
    "    gpt4 = True\n",
    "  print (model[\"id\"])\n",
    "\n",
    "if gpt4:\n",
    "  print (\"You have GPT 4!\")\n",
    "else:\n",
    "  print (\"Sorry, not today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-6xM8ZPgpLR9QneyTrdsmxRm0XqGye at 0x117f8eb10> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Hello there! How can I be of assistance?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1679604467,\n",
       "  \"id\": \"chatcmpl-6xM8ZPgpLR9QneyTrdsmxRm0XqGye\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 10,\n",
       "    \"prompt_tokens\": 11,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's make a very simple prompt.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\", \n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello world!\"}]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Learning\n",
    "\n",
    "Now we'll take a look at how to give GPT a few examples of what we want. This can be an incredibly powerful technique to get exactly what you're looking for out of the model.\n",
    "\n",
    "The system message sets the broad parameters for how you want GPT to ask, then a series of prompts between user and assistant show GPT exactly how you want it to act. The final message comes from \"user\", prompting GPT to return a final \"assistant\" message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x135487ce0> JSON: {\n",
       "  \"content\": \"positive\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's make a very simple prompt.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\", \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a sentiment analysis assitant. Anything I say, I want you to classify it as positive, neutral, or negative. Just give one of those words, based on your analysis of the text, and nothing else.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This is great!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"positive\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm going to the store\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"neutral\"},\n",
    "        {\"role\": \"user\", \"content\": \"This is terrible!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
    "        {\"role\": \"user\", \"content\": \"This is pretty nice.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Memory\n",
    "\n",
    "Please note that the GPT API has no memory in the way that ChatGPT does in the UI. To build memory into your own applications, you can use the same format above to capture previous user and assistant messages and then send them back with each call. This will give GPT the same context as if you were using the ChatGPT UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
