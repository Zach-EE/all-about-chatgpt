{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple overview of the OpenAI SDK in Python\n",
    "\n",
    "Note: You need to set your API Key first in your environment like so:\n",
    "\n",
    "`export OPENAI_API_KEY=<api_key>`\n",
    "\n",
    "And of course install [the Library](https://github.com/openai/openai-python):\n",
    "\n",
    "`pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Models: ['babbage', 'davinci', 'text-davinci-edit-001', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'ada', 'curie-instruct-beta', 'babbage-code-search-text', 'babbage-similarity', 'whisper-1', 'code-search-babbage-text-001', 'text-curie-001', 'code-search-babbage-code-001', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'ada-code-search-code', 'ada-similarity', 'text-davinci-003', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'gpt-3.5-turbo-0301', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'gpt-3.5-turbo', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'gpt-4', 'text-search-davinci-doc-001', 'gpt-4-0314', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n",
      "You have GPT 4!\n",
      "gpt-4\n",
      "gpt-4-0314\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import openai\n",
    "\n",
    "# list models\n",
    "models = openai.Model.list()\n",
    "model_names = [m[\"id\"] for m in models.data]\n",
    "print(f\"All Models: {model_names}\")\n",
    "\n",
    "# Now let's take a look at the models and see if we have GPT-4 yet or not.\n",
    "gpt4 = []\n",
    "for model in models.data: \n",
    "  if 'gpt-4' in model[\"id\"]:\n",
    "    gpt4.append(model[\"id\"])\n",
    "\n",
    "if gpt4:\n",
    "  print (\"You have GPT 4!\")\n",
    "  for model in gpt4:\n",
    "    print(model)\n",
    "else:\n",
    "  print (\"Sorry, not today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-6zOzU1QHjwX88iLErinEWGFkqPcCj at 0x135509fd0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Hello! How can I help you today?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1680092092,\n",
       "  \"id\": \"chatcmpl-6zOzU1QHjwX88iLErinEWGFkqPcCj\",\n",
       "  \"model\": \"gpt-4-0314\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 9,\n",
       "    \"prompt_tokens\": 10,\n",
       "    \"total_tokens\": 19\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's make a very simple prompt.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello world!\"}]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Learning\n",
    "\n",
    "Now we'll take a look at how to give GPT a few examples of what we want. This can be an incredibly powerful technique to get exactly what you're looking for out of the model.\n",
    "\n",
    "The system message sets the broad parameters for how you want GPT to ask, then a series of prompts between user and assistant show GPT exactly how you want it to act. The final message comes from \"user\", prompting GPT to return a final \"assistant\" message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x135487ce0> JSON: {\n",
       "  \"content\": \"positive\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's make a very simple prompt.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\", \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a sentiment analysis assitant. Anything I say, I want you to classify it as positive, neutral, or negative. Just give one of those words, based on your analysis of the text, and nothing else.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This is great!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"positive\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm going to the store\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"neutral\"},\n",
    "        {\"role\": \"user\", \"content\": \"This is terrible!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
    "        {\"role\": \"user\", \"content\": \"This is pretty nice.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Memory\n",
    "\n",
    "Please note that the GPT API has no memory in the way that ChatGPT does in the UI. To build memory into your own applications, you can use the same format above to capture previous user and assistant messages and then send them back with each call. This will give GPT the same context as if you were using the ChatGPT UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
